{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1553d57",
   "metadata": {},
   "source": [
    "## Web Scraping Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4624af01",
   "metadata": {},
   "source": [
    "### ประเด็นปัญหา\n",
    "บริษัทขาดข้อมูลเพียงพอในการตัดสินใจว่า ควรสต็อกหนังสือประเภทใดที่จะสร้างรายได้สูงสุด การขาดข้อมูลเชิงลึกเกี่ยวกับแนวโน้มราคาหนังสือ ความชอบของลูกค้า และข้อเสนอจากคู่แข่ง ทำให้ไม่สามารถวางแผนกลยุทธ์ได้อย่างแม่นยำ การวิเคราะห์ข้อมูลจากแหล่งข้อมูลออนไลน์จึงเป็นวิธีที่มีประสิทธิภาพในการช่วยเหลือบริษัทในการตัดสินใจ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86af63",
   "metadata": {},
   "source": [
    "### วัตถุประสงค์\n",
    "ร้านขายหนังสือออนไลน์สตาร์ทอัพต้องการปรับกลยุทธ์การเลือกสต็อกหนังสือให้มีความสอดคล้องกับแนวโน้มของตลาดในปัจจุบัน เป้าหมายหลักของการวิเคราะห์ข้อมูลครั้งนี้คือการช่วยให้บริษัทตัดสินใจเกี่ยวกับการเลือกหนังสือสต็อกที่สามารถเพิ่มผลตอบแทนจากการลงทุน เบื้องต้นได้อย่างมีประสิทธิภาพ การทำเช่นนี้จะช่วยให้ธุรกิจเติบโตในตลาดที่มีการแข่งขันสูง"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2e450",
   "metadata": {},
   "source": [
    "### ความสำคัญของการวิเคราะห์ข้อมูล\n",
    "ข้อมูลจากการจัดอันดับหนังสือ บทวิจารณ์ แนวโน้มราคา และรูปแบบการสต็อกหนังสือ เป็นปัจจัยสำคัญที่สามารถให้ข้อมูลเชิงลึกเกี่ยวกับความต้องการของตลาด การวิเคราะห์ข้อมูลเหล่านี้จะช่วยให้ร้านสามารถเข้าใจแนวโน้มและความชอบของลูกค้าได้ดีขึ้น รวมถึงการใช้ข้อมูลดังกล่าวในการตัดสินใจเรื่องราคาหนังสือ การจัดการสต็อก และการวางกลยุทธ์การขายในระยะยาวได้"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e757b42",
   "metadata": {},
   "source": [
    "ขั้นตอนที่ 1 การดึงข้อมูลจากเว็บ (Web Scraping) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import web grabbing client and\n",
    "# HTML parser\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    " \n",
    "# variable to store website link as string\n",
    "myurl = 'http://books.toscrape.com/index.html'\n",
    " \n",
    "# grab website and store in variable uclient\n",
    "uClient = uReq(myurl)\n",
    " \n",
    "# read and close HTML\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    " \n",
    "# call BeautifulSoup for parsing\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    " \n",
    "# grabs all the products under list tag\n",
    "bookshelf = page_soup.findAll(\n",
    "    \"li\", {\"class\": \"col-xs-6 col-sm-4 col-md-3 col-lg-3\"})\n",
    " \n",
    "# create csv file of all products\n",
    "filename = (\"Books1.csv\")\n",
    "f = open(filename, \"w\")\n",
    " \n",
    "headers = \"Book title, Price\\n\"\n",
    "f.write(headers)\n",
    " \n",
    "for books in bookshelf:\n",
    " \n",
    "    # collect title of all books\n",
    "    book_title = books.h3.a[\"title\"]\n",
    " \n",
    "    # collect book price of all books\n",
    "    book_price = books.findAll(\"p\", {\"class\": \"price_color\"})\n",
    "    price = book_price[0].text.strip()\n",
    "\n",
    "            \n",
    "    # If book_title or book_price contains a comma, enclose in quotes\n",
    "    if \",\" in book_title:\n",
    "        book_title = f'\"{book_title}\"'\n",
    "    if \",\" in book_price:\n",
    "        book_price = f'\"{book_price}\"'\n",
    " \n",
    "    print(\"Title of the book :\" + book_title)\n",
    "    print(\"Price of the book :\" + price)\n",
    " \n",
    "    # f.write(book_title + \",\" + price+\"\\n\")\n",
    " \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517efac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import web grabbing client and\n",
    "# HTML parser\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import csv\n",
    " \n",
    "# variable to store website link as string\n",
    "myurl = 'http://books.toscrape.com/index.html'\n",
    " \n",
    "# grab website and store in variable uclient\n",
    "uClient = uReq(myurl)\n",
    " \n",
    "# read and close HTML\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "\n",
    "# call BeautifulSoup for parsing\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    " \n",
    "# grabs all the products under list tag\n",
    "bookshelf = page_soup.findAll( \"li\", {\"class\": \"col-xs-6 col-sm-4 col-md-3 col-lg-3\"})\n",
    " \n",
    "# create csv file of all products\n",
    "filename = (\"Books2.csv\")\n",
    "f = open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\")\n",
    "writer = csv.writer(f)\n",
    " \n",
    "writer.writerow([\"Book Title\", \"Price\"])\n",
    " \n",
    "for books in bookshelf:\n",
    " \n",
    "    # collect title of all books\n",
    "    book_title = books.h3.a[\"title\"]\n",
    " \n",
    "    # collect book price of all books\n",
    "    book_price = books.findAll(\"p\", {\"class\": \"price_color\"})\n",
    "    price = book_price[0].text.strip()\n",
    " \n",
    "    print(\"Title of the book :\" + book_title)\n",
    "    print(\"Price of the book :\" + price)\n",
    " \n",
    "    writer.writerow([book_title, price])\n",
    " \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d44ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://books.toscrape.com/'\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Assuming categories are listed in a sidebar or specific section\n",
    "category_container = soup.find('div', {'class': 'side_categories'})\n",
    "categories = category_container.find_all('a')\n",
    "\n",
    "Categories = []\n",
    "\n",
    "for category in categories:\n",
    "    category_name = category.get_text().strip()\n",
    "    category_link = url + category['href']\n",
    "    # print(category_name, category_link)\n",
    "    Categories.append({\"Category\":category_name,\"Link\":category_link})\n",
    "\n",
    "for c in Categories:\n",
    "    print(c['Category'],c[\"Link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea567d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.parse import urljoin  # Import the urljoin function\n",
    "\n",
    "Extracted_links = []\n",
    "\n",
    "for i in range(49):\n",
    "    # Correct the base URL to the root of the book listings\n",
    "    base_url = f'https://books.toscrape.com/catalogue/category/books_1/page-{i + 1}.html'\n",
    "\n",
    "    # Open connection and grab the main page\n",
    "    uClient = uReq(base_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "\n",
    "    # HTML parsing\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "    # Find the correct link by inspecting where the relative links start\n",
    "    for bookLink in page_soup.findAll(\"h3\"):\n",
    "       Extracted_links.append(urljoin(base_url, bookLink.find('a')['href']))\n",
    "\n",
    "# Print the correctly formed URLs\n",
    "for link in Extracted_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "number_mapping = {\n",
    "\"One\": 1,\n",
    "\"Two\": 2,\n",
    "\"Three\": 3,\n",
    "\"Four\": 4,\n",
    "\"Five\": 5\n",
    "}   \n",
    "\n",
    "# Test_links = [\"https://books.toscrape.com/catalogue/hawkeye-vol-1-my-life-as-a-weapon-hawkeye-1_24/index.html\",\"https://books.toscrape.com/catalogue/having-the-barbarians-baby-ice-planet-barbarians-75_23/index.html\"]\n",
    "\n",
    "# Create CSV file of all products\n",
    "filename = \"Books_data.csv\"\n",
    "with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write header row\n",
    "    writer.writerow([\"Genre\", \"Book Title\", \"Price\", \"Rating\", \"Stock Status\", \"Number of Reviews\"])\n",
    "\n",
    "    # Loop through each collected book link to access detail pages\n",
    "    for link in Extracted_links:\n",
    "        # Fetch the detail page\n",
    "        response = requests.get(link)\n",
    "        detail_html = response.text\n",
    "        \n",
    "        # HTML parsing for detail page\n",
    "        detail_soup = soup(detail_html, \"html.parser\")\n",
    "\n",
    "        # Extracting detailed product information\n",
    "        product_info = {row.th.text: row.td.text for row in detail_soup.findAll(\"tr\")}\n",
    "        category = detail_soup.find('ul', class_=\"breadcrumb\").find_all('a')[2].text.strip()\n",
    "        title = detail_soup.find('h1').text\n",
    "        price = product_info.get(\"Price (incl. tax)\")\n",
    "        rating = number_mapping.get(detail_soup.find('p', class_=\"star-rating\")['class'][1], \"Invalid input\")\n",
    "        stock_status = product_info.get(\"Availability\")\n",
    "        number_of_reviews = product_info.get(\"Number of reviews\")\n",
    "\n",
    "        # Write the product details to the CSV\n",
    "        writer.writerow([category, title, price, rating, stock_status, number_of_reviews])\n",
    "\n",
    "        print(title, \"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71d729",
   "metadata": {},
   "source": [
    "ขั้นตอนที่ 2 การทำความสะอาดและเตรียมข้อมูล (Data Cleaning and Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3719a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Books_data.csv')\n",
    "\n",
    "print(df.info())   # ตรวจสอบชนิดของข้อมูลในแต่ละคอลัมน์และจำนวนข้อมูลที่ไม่เป็น null\n",
    "print(df.duplicated().sum())    # ตรวจสอบข้อมูลที่ซ้ำกัน\n",
    "df['Price'] = df['Price'].str.replace('[Ã‚Â£]', '', regex=True) #ลบสกุลเงินออก\n",
    "df['Stock Status'] = df['Stock Status'].str.replace(r'\\D+', '', regex=True) #ลบstr\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n",
    "df = df[(df['Rating'] >= 1) & (df['Rating'] <= 5)]  # ตรวจสอบค่า Rating ที่ไม่อยู่ในช่วงที่เหมาะสม (1-5)\n",
    "df = df[df['Price'] > 0]    # ตรวจสอบราคาหนังสือที่อาจไม่สมเหตุสมผล\n",
    "df = df.drop(columns = \"Number of Reviews\")\n",
    "\n",
    "duplicate_count = df.duplicated(subset=['Book Title']).sum()\n",
    "print(duplicate_count)\n",
    "\n",
    "# ตรวจสอบแถวที่มีชื่อซ้ำ\n",
    "duplicates = df[df.duplicated(subset=['Book Title'], keep=False)]\n",
    "# หาและนับชื่อที่ซ้ำ\n",
    "duplicate_titles = duplicates['Book Title'].value_counts()\n",
    "print(duplicate_titles)\n",
    "\n",
    "\n",
    "df.to_csv('cleaned_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f841d54",
   "metadata": {},
   "source": [
    "ขั้นตอนที่ 3 การวิเคราะห์ข้อมูล (Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f406ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "# โหลดข้อมูลที่ cleanแล้ว\n",
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e952283-ec67-41a6-a4dc-b0f58689d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#จัดอันดับความนิยมของหนังสือแต่ล่ะประเภทและ หาค่าเฉลี่ยของ price ตามประเภท\n",
    "genre_mean_price_rating = df.groupby('Genre')[['Rating','Price']].mean()\n",
    "ranking_genre_byRating = genre_mean_price_rating.sort_values(by='Rating', ascending=False)\n",
    "ranking_genre_byRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e6c6f-2242-4fce-9845-41a0684e4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#การทดสอบ ANOVA สำหรับความแตกต่างของราคาตาม Genre\n",
    "f_val, p_val = stats.f_oneway(*(df[df['Genre'] == genre]['Price'] for genre in df['Genre'].unique()))\n",
    "print(\"F-Value of genre:\", f_val)\n",
    "print(\"P-Value of genre:\", p_val)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#การทดสอบ ANOVA สำหรับความแตกต่างของราคาตาม rating\n",
    "df['Rating_Category'] = pd.cut(df['Rating'],\n",
    "                               bins=[0, 2, 4, 5],  # กำหนดช่วงคะแนน\n",
    "                               labels=['Low', 'Medium', 'High'])  # ตั้งชื่อกลุ่ม\n",
    "# สร้างลิสต์ของข้อมูลราคาในแต่ละกลุ่มของ Rating\n",
    "price_by_rating = [df[df['Rating_Category'] == category]['Price'] for category in df['Rating_Category'].unique()]\n",
    "\n",
    "# ทำการทดสอบ ANOVA\n",
    "f_val, p_val = stats.f_oneway(*price_by_rating)\n",
    "\n",
    "print(\"F-Value of rating:\", f_val)\n",
    "print(\"P-Value of rating:\", p_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3c6ac",
   "metadata": {},
   "source": [
    "ขั้นตอนที่ 4 การสร้างภาพแสดงข้อมูล (Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# แสดงการกระจายของ Stock Status ตาม Rating\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y='Stock Status', x='Rating', data=df)\n",
    "plt.title('Stock Status vs Rating')\n",
    "plt.show()\n",
    "\n",
    "# การวิเคราะห์ความสัมพันธ์ของ Stock Status กับ Genre\n",
    "stock_status_genre = df.groupby('Genre')['Stock Status'].mean()\n",
    "\n",
    "# print(\"Average Stock Status by Genre:\")\n",
    "# print(stock_status_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.bar(ranking_genre_byRating.index, ranking_genre_byRating['Rating'], color='b', alpha=0.7, label='Rating')\n",
    "ax1.set_xlabel('Genre')\n",
    "ax1.set_ylabel('Average Rating', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49523ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.plot(ranking_genre_byRating.index, ranking_genre_byRating['Price'], color='r', marker='o', label='Price')\n",
    "ax1.set_xlabel('Genre')\n",
    "ax1.set_ylabel('Average Price', color='r')\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "plt.title('Average Price by Genre')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b49d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.bar(ranking_genre_byRating.index, ranking_genre_byRating['Rating'], color='b', alpha=0.7, label='Rating')\n",
    "ax1.set_xlabel('Genre')\n",
    "ax1.set_ylabel('Average Rating', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Visualization')\n",
    "\n",
    "ax1.bar(ranking_genre_byRating.index, ranking_genre_byRating['Rating'], color='b', alpha=0.7, label='Rating')\n",
    "ax1.set_xlabel('Genre')\n",
    "ax1.set_ylabel('Average Rating', color='b')\n",
    "ax1.tick_params(axis='y', labelcolor='b')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Visualization')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(ranking_genre_byRating.index, ranking_genre_byRating['Price'], color='r', marker='o', label='Price')\n",
    "ax2.set_ylabel('Average Price', color='r')\n",
    "ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb1a21",
   "metadata": {},
   "source": [
    "## รายชื่อสมาชิกในกลุ่ม\n",
    "| ลำดับ | ชื่อ          | รหัสนิสิต |\n",
    "|--------|-------------|------------------|\n",
    "| 1      | ชื่อ นามสกุล  | xxxxxxxxxx       |\n",
    "| 2      | ชื่อ นามสกุล  | yyyyyyyyyy       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
